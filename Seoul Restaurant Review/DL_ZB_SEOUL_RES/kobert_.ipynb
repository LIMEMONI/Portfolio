{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBERT를 이용해 학습적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ysj_1\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ysj_1\\miniconda3\\envs\\ds_study\\lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ysj_1\\miniconda3\\envs\\ds_study\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_star.to_csv('df_star.csv',index=False)\n",
    "df_no_star.to_csv('df_no_star.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_star = pd.read_csv('df_star.csv')\n",
    "df_no_star = pd.read_csv('df_no_star.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_star = df_star[['review_content',\t'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_star = df_no_star[['review_content',\t'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_star.reset_index(drop=True, inplace=True)\n",
    "df_no_star.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_star 데이터프레임을 train과 test로 나누기 위해 train_test_split 함수를 사용한다.\n",
    "# test_size는 test 데이터셋의 비율을 의미한다.\n",
    "# random_state는 난수 발생 시드값으로, 같은 시드값을 사용하면 같은 결과를 얻을 수 있다.\n",
    "train, test = train_test_split(df_star, test_size=0.2, random_state=42,stratify=df_star['rating'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True,inplace=True)\n",
    "test.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197857, 49465)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    43125\n",
       " 0     6340\n",
       " Name: rating, dtype: int64,\n",
       " 1    172496\n",
       " 0     25361\n",
       " Name: rating, dtype: int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['rating'].value_counts(),train['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', } \n",
    "\n",
    "\n",
    "def clean(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_str(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[^\\w\\s\\n]'         # 특수기호제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','', string=text)\n",
    "    text = re.sub('\\n', '.', string=text)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_star = df_star.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>최고입니다 수년째 단골인데 리뷰는 처음 써보네요 이곳저곳에서 장어 많이 먹어봤는데 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>맛은 좋은데  일하시는 여자 분 진짜 매너 없음  잠바 입고 써빙해줌  목도리 자락...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>양평 유명 장어맛집이라고 가봤던곳은 흙냄새 같은게 나서 많이 먹을수가 없었는데 여기...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>맛있어요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>상봉역 맛집 쿠시꼬시 소금구이 추천합니다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247317</th>\n",
       "      <td>주인할머니께서 맛있게 해주세요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247318</th>\n",
       "      <td>비빔국수가 빨간양념 맛보다 참기름 맛이 좀 더 강한 비빔국수예요  그래서 같이 주신...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247319</th>\n",
       "      <td>국수 맛나서 종종 방문합니다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247320</th>\n",
       "      <td>멸치국수 삼성역 구수하고 정겨운 맛이에요   곱빼기도 파는데 담에는 곱빼기 먹을래요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247321</th>\n",
       "      <td>병원앞 작은 국수집인데 왕만두 떡국먹었는데 맛있었어요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247322 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_content  rating\n",
       "0       최고입니다 수년째 단골인데 리뷰는 처음 써보네요 이곳저곳에서 장어 많이 먹어봤는데 ...       1\n",
       "1       맛은 좋은데  일하시는 여자 분 진짜 매너 없음  잠바 입고 써빙해줌  목도리 자락...       0\n",
       "2       양평 유명 장어맛집이라고 가봤던곳은 흙냄새 같은게 나서 많이 먹을수가 없었는데 여기...       1\n",
       "3                                                    맛있어요       1\n",
       "4                                  상봉역 맛집 쿠시꼬시 소금구이 추천합니다       1\n",
       "...                                                   ...     ...\n",
       "247317                                  주인할머니께서 맛있게 해주세요        1\n",
       "247318  비빔국수가 빨간양념 맛보다 참기름 맛이 좀 더 강한 비빔국수예요  그래서 같이 주신...       1\n",
       "247319                                   국수 맛나서 종종 방문합니다        1\n",
       "247320    멸치국수 삼성역 구수하고 정겨운 맛이에요   곱빼기도 파는데 담에는 곱빼기 먹을래요        1\n",
       "247321                      병원앞 작은 국수집인데 왕만두 떡국먹었는데 맛있었어요       1\n",
       "\n",
       "[247322 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_star.loc[:,'review_content'] = df_star['review_content'].apply(lambda x : clean_str(x)).copy()\n",
    "df_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>밑반찬도 맛있고 특히 장어가 맛있어요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>맛있어요 반찬도 맛있구요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>좋아요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>좋아요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>맛있어용</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011987</th>\n",
       "      <td>뇸뇸</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011988</th>\n",
       "      <td>맛납니다.</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011989</th>\n",
       "      <td>NaN</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011990</th>\n",
       "      <td>맛있어요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011991</th>\n",
       "      <td>맛있어요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011992 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               review_content rating\n",
       "0        밑반찬도 맛있고 특히 장어가 맛있어요   별점없음\n",
       "1               맛있어요 반찬도 맛있구요   별점없음\n",
       "2                         좋아요   별점없음\n",
       "3                        좋아요    별점없음\n",
       "4                        맛있어용   별점없음\n",
       "...                       ...    ...\n",
       "1011987                    뇸뇸   별점없음\n",
       "1011988                 맛납니다.   별점없음\n",
       "1011989                   NaN   별점없음\n",
       "1011990                  맛있어요   별점없음\n",
       "1011991                  맛있어요   별점없음\n",
       "\n",
       "[1011992 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>밑반찬도 맛있고 특히 장어가 맛있어요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>맛있어요 반찬도 맛있구요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>좋아요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>좋아요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>맛있어용</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011973</th>\n",
       "      <td>먹어본것중최고입돠.또먹구시퍼요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011974</th>\n",
       "      <td>신도림에서 곱창 먹고 싶으면 이리로 옵니다.신선하고 맛있어요</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011977</th>\n",
       "      <td>최악의 경험 사람 없는 시간대에 갔음에도 최악이었던 서비스와 심지어 맛까지 없었던 ...</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011979</th>\n",
       "      <td>6년째 다니고 있는 내 최애곱창님 언제나 맛있고 그 맛에 감동과 함께 과음하게 됩니다</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011982</th>\n",
       "      <td>서울곱창집중원탑이라고생각합니다 멀어서가끔오지만역시변함없이신선하고맛있어여 최애곱창집</td>\n",
       "      <td>별점없음</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328451 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_content rating\n",
       "0                                     밑반찬도 맛있고 특히 장어가 맛있어요   별점없음\n",
       "1                                            맛있어요 반찬도 맛있구요   별점없음\n",
       "2                                                      좋아요   별점없음\n",
       "3                                                     좋아요    별점없음\n",
       "4                                                     맛있어용   별점없음\n",
       "...                                                    ...    ...\n",
       "1011973                                   먹어본것중최고입돠.또먹구시퍼요   별점없음\n",
       "1011974                  신도림에서 곱창 먹고 싶으면 이리로 옵니다.신선하고 맛있어요   별점없음\n",
       "1011977  최악의 경험 사람 없는 시간대에 갔음에도 최악이었던 서비스와 심지어 맛까지 없었던 ...   별점없음\n",
       "1011979    6년째 다니고 있는 내 최애곱창님 언제나 맛있고 그 맛에 감동과 함께 과음하게 됩니다   별점없음\n",
       "1011982      서울곱창집중원탑이라고생각합니다 멀어서가끔오지만역시변함없이신선하고맛있어여 최애곱창집   별점없음\n",
       "\n",
       "[328451 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_star = df_no_star.drop_duplicates(subset='review_content')\n",
    "df_no_star.dropna(inplace=True)\n",
    "df_no_star.loc[:,'review_content'] = df_no_star['review_content'].apply(lambda x : clean_str(x)).copy()\n",
    "df_no_star"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 버트 인풋 만들기\n",
    "\n",
    "한글 데이터를 분석하려면, 100개가 넘는 언어에 대해 훈련된 버트를 사용해야 합니다.\n",
    "이번에는 한국어 데이터로 훈련되었고, SKT에서 만든 KoBERT를 사용하도록 하겠습니다.\n",
    "모델을 로드하기에 앞서, 토크나이저를 불러오도록 하겠습니다.\n",
    "huggingface에서는 아주 쉽게 토크나이저를 불러올 수 있습니다.\n",
    "https://github.com/monologg/KoBERT-NER 에서 kobert를 tokenize 할 수 있는 코드를 가져왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT 토크나이저를 활용 \n",
    "\n",
    "import logging\n",
    "import os\n",
    "import unicodedata\n",
    "from shutil import copyfile\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
    "                     \"vocab_txt\": \"vocab.txt\"}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
    "    },\n",
    "    \"vocab_txt\": {\n",
    "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
    "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
    "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"monologg/kobert\": 512,\n",
    "    \"monologg/kobert-lm\": 512,\n",
    "    \"monologg/distilkobert\": 512\n",
    "}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
    "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
    "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = u'▁'\n",
    "\n",
    "\n",
    "class KoBertTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "        SentencePiece based tokenizer. Peculiarities:\n",
    "            - requires `SentencePiece `_\n",
    "    \"\"\"\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_file,\n",
    "            vocab_txt,\n",
    "            do_lower_case=False,\n",
    "            remove_space=True,\n",
    "            keep_accents=False,\n",
    "            unk_token=\"[UNK]\",\n",
    "            sep_token=\"[SEP]\",\n",
    "            pad_token=\"[PAD]\",\n",
    "            cls_token=\"[CLS]\",\n",
    "            mask_token=\"[MASK]\",\n",
    "            **kwargs):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Build vocab\n",
    "        self.token2idx = dict()\n",
    "        self.idx2token = []\n",
    "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
    "            for idx, token in enumerate(f):\n",
    "                token = token.strip()\n",
    "                self.token2idx[token] = idx\n",
    "                self.idx2token.append(token)\n",
    "\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.remove_space = remove_space\n",
    "        self.keep_accents = keep_accents\n",
    "        self.vocab_file = vocab_file\n",
    "        self.vocab_txt = vocab_txt\n",
    "\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(vocab_file)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.idx2token)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "        except ImportError:\n",
    "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
    "                           \"pip install sentencepiece\")\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.Load(self.vocab_file)\n",
    "\n",
    "    def preprocess_text(self, inputs):\n",
    "        if self.remove_space:\n",
    "            outputs = \" \".join(inputs.strip().split())\n",
    "        else:\n",
    "            outputs = inputs\n",
    "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
    "\n",
    "        if not self.keep_accents:\n",
    "            outputs = unicodedata.normalize('NFKD', outputs)\n",
    "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
    "        if self.do_lower_case:\n",
    "            outputs = outputs.lower()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
    "        \"\"\" Tokenize a string. \"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "\n",
    "        if not sample:\n",
    "            pieces = self.sp_model.EncodeAsPieces(text)\n",
    "        else:\n",
    "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
    "        new_pieces = []\n",
    "        for piece in pieces:\n",
    "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
    "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
    "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "                    if len(cur_pieces[0]) == 1:\n",
    "                        cur_pieces = cur_pieces[1:]\n",
    "                    else:\n",
    "                        cur_pieces[0] = cur_pieces[0][1:]\n",
    "                cur_pieces.append(piece[-1])\n",
    "                new_pieces.extend(cur_pieces)\n",
    "            else:\n",
    "                new_pieces.append(piece)\n",
    "\n",
    "        return new_pieces\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index, return_unicode=True):\n",
    "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
    "        return self.idx2token[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
    "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A KoBERT sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A KoBERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
    "            to a directory.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        # 1. Save sentencepiece model\n",
    "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
    "            copyfile(self.vocab_file, out_vocab_model)\n",
    "\n",
    "        # 2. Save vocab.txt\n",
    "        index = 0\n",
    "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
    "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return out_vocab_model, out_vocab_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer_78b3253a26.model from cache at C:\\Users\\ysj_1/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\tokenizer_78b3253a26.model\n",
      "loading file vocab.txt from cache at C:\\Users\\ysj_1/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\ysj_1/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\ysj_1/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"monologg/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 197857/197857 [00:29<00:00, 6797.50it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "\n",
    "    SEQ_LEN = 64  # SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
    "\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        # token : 문장을 토큰화함\n",
    "        token = tokenizer.encode(\n",
    "            data_df[DATA_COLUMN][i], truncation=True, padding='max_length', max_length=SEQ_LEN)\n",
    "\n",
    "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "\n",
    "        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        segments.append(segment)\n",
    "\n",
    "        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n",
    "        targets.append(data_df[LABEL_COLUMN][i])\n",
    "\n",
    "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return [tokens, masks, segments], targets\n",
    "\n",
    "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
    "\n",
    "\n",
    "def load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "    return data_x, data_y\n",
    "\n",
    "\n",
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "# 긍부정 문장을 포함하고 있는 칼럼\n",
    "DATA_COLUMN = \"review_content\"\n",
    "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n",
    "LABEL_COLUMN = \"rating\"\n",
    "\n",
    "# train 데이터를 버트 인풋에 맞게 변환\n",
    "train_x, train_y = load_data(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   2, 3194, 5561, ...,    1,    1,    1],\n",
       "        [   2, 4996, 1967, ...,    1,    1,    1],\n",
       "        [   2,  517, 5960, ..., 6903, 3255,    3],\n",
       "        ...,\n",
       "        [   2,  517,    9, ...,    1,    1,    1],\n",
       "        [   2, 2482, 5859, ...,    1,    1,    1],\n",
       "        [   2,  517, 6831, ...,    1,    1,    1]]),\n",
       " array([[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20540/20540 [00:03<00:00, 6194.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 성능을 검증한 test 데이터를 버트 인풋에 맞게 변환\n",
    "test_x, test_y = load_data(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 버트를 활용한 감성분석 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\ysj_1/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\ysj_1/.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\pytorch_model.bin\n",
      "Loading PyTorch weights from C:\\Users\\ysj_1\\.cache\\huggingface\\hub\\models--monologg--kobert\\snapshots\\8ebf2818cfd85570737d31ed8cd7aaa000e7056c\\pytorch_model.bin\n",
      "PyTorch checkpoint contains 92,186,880 parameters\n",
      "Loaded 92,186,880 parameters in the TF 2.0 model.\n",
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(\n",
    "    \"monologg/kobert\", from_pt=True)  # monologg/kobert 모델 불러오기\n",
    "\n",
    "\n",
    "# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
    "token_inputs = tf.keras.layers.Input(\n",
    "    (SEQ_LEN,), dtype=tf.int32, name='input_word_ids')  # 입력 토큰 정의\n",
    "mask_inputs = tf.keras.layers.Input(\n",
    "    (SEQ_LEN,), dtype=tf.int32, name='input_masks')  # 입력 마스크 정의\n",
    "segment_inputs = tf.keras.layers.Input(\n",
    "    (SEQ_LEN,), dtype=tf.int32, name='input_segment')  # 입력 세그먼트 정의\n",
    "\n",
    "\n",
    "# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
    "bert_outputs = model(\n",
    "    [token_inputs, mask_inputs, segment_inputs])  # bert 모델의 출력 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<KerasTensor: shape=(None, 64, 768) dtype=float32 (created by layer 'tf_bert_model')>, pooler_output=<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model')>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs = bert_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ysj_1\\miniconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\ysj_1\\miniconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "c:\\Users\\ysj_1\\miniconda3\\envs\\ds_study\\lib\\site-packages\\tensorflow_addons\\optimizers\\rectified_adam.py:121: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Rectified Adam 옵티마이저 사용\n",
    "# tensorflow_addons 패키지 설치\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# 총 batch size * 4 epoch = 2344 * 4\n",
    "# Rectified Adam 옵티마이저 생성\n",
    "opt = tfa.optimizers.RectifiedAdam(\n",
    "    lr=5.0e-5,  # learning rate 설정\n",
    "    total_steps=2344*2,  # 총 스텝 수 설정\n",
    "    warmup_proportion=0.1,  # warmup 비율 설정\n",
    "    # warmup 스텝 수 설정\n",
    "    #warmup_steps=int((2344*2) * 0.1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_drop = tf.keras.layers.Dropout(\n",
    "    0.5)(bert_outputs)  # BERT 출력값에 드롭아웃 적용\n",
    "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(\n",
    "    stddev=0.02))(sentiment_drop)  # 드롭아웃 적용된 BERT 출력값에 fully connected layer 적용\n",
    "sentiment_model = tf.keras.Model(\n",
    "    [token_inputs, mask_inputs, segment_inputs], sentiment_first)  # 입력값과 출력값을 지정하여 모델 생성\n",
    "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=[\n",
    "                        'acc'])  # 모델 컴파일, optimizer는 Adam, loss는 binary crossentropy, 평가 지표는 accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1284/1284 [==============================] - 21795s 17s/step - loss: 0.3179 - acc: 0.8812 - val_loss: 0.2222 - val_acc: 0.9168\n",
      "Epoch 2/2\n",
      "1284/1284 [==============================] - 22433s 17s/step - loss: 0.2081 - acc: 0.9232 - val_loss: 0.2108 - val_acc: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff1273e970>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU를 사용하기 위해 런타임 유형을 GPU로 변경\n",
    "\n",
    "# 필요한 라이브러리 import\n",
    "import tensorflow as tf\n",
    "\n",
    "# GPU를 사용하기 위한 설정\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "# 모델 학습\n",
    "with tf.device('/device:GPU:0'):\n",
    "  sentiment_model.fit(train_x, train_y, epochs=2, shuffle=True,\n",
    "                      batch_size=64, validation_data=(test_x, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_convert_data(data_df):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(data_df))):  # 데이터프레임의 길이만큼 반복문 실행\n",
    "\n",
    "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN,\n",
    "                                 truncation=True, padding='max_length')  # 문장을 토큰화하여 인코딩\n",
    "        num_zeros = token.count(0)  # 0의 개수를 세어 num_zeros에 저장\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros  # mask 생성\n",
    "        segment = [0]*SEQ_LEN  # segment 생성\n",
    "\n",
    "        tokens.append(token)  # tokens 리스트에 token 추가\n",
    "        segments.append(segment)  # segments 리스트에 segment 추가\n",
    "        masks.append(mask)  # masks 리스트에 mask 추가\n",
    "\n",
    "    tokens = np.array(tokens)  # tokens 리스트를 numpy 배열로 변환\n",
    "    masks = np.array(masks)  # masks 리스트를 numpy 배열로 변환\n",
    "    segments = np.array(segments)  # segments 리스트를 numpy 배열로 변환\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
    "\n",
    "\n",
    "def predict_load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(\n",
    "        str)  # 데이터프레임의 DATA_COLUMN 열을 문자열로 변환\n",
    "    data_x = predict_convert_data(data_df)  # convert_data 함수를 이용하여 데이터 전처리\n",
    "    return data_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20540/20540 [00:04<00:00, 4751.73it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_set = predict_load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   2, 1967, 7086, ...,    1,    1,    1],\n",
       "        [   2,  517, 6040, ...,    1,    1,    1],\n",
       "        [   2,  532, 5789, ...,    1,    1,    1],\n",
       "        ...,\n",
       "        [   2, 1967, 7141, ...,    1,    1,    1],\n",
       "        [   2, 1458, 1967, ...,    1,    1,    1],\n",
       "        [   2, 4656,   54, ...,    1,    1,    1]]),\n",
       " array([[1, 1, 1, ..., 1, 1, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sentiment_model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46574444],\n",
       "       [0.9099399 ],\n",
       "       [0.3828514 ],\n",
       "       ...,\n",
       "       [0.889914  ],\n",
       "       [0.9962193 ],\n",
       "       [0.99392766]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 부정이면 0, 긍정이면 1 출력\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.54      0.64      2746\n",
      "           1       0.93      0.98      0.95     17794\n",
      "\n",
      "    accuracy                           0.92     20540\n",
      "   macro avg       0.86      0.76      0.80     20540\n",
      "weighted avg       0.91      0.92      0.91     20540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = test['rating']\n",
    "# F1 Score 확인\n",
    "print(classification_report(y_true, np.round(preds,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 긍정에 대해서는 스코어가 높게 나오나 부정에 대해서는 스코어가 낮게 나온다.\n",
    "- 데이터의 불균형의 문제가 있는 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    tokens, masks, segments = [], [], []\n",
    "    token = tokenizer.encode(data, max_length=SEQ_LEN,\n",
    "                             truncation=True, padding='max_length')\n",
    "\n",
    "    num_zeros = token.count(0)  # 패딩으로 추가된 0의 개수를 세어준다.\n",
    "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros  # 패딩 부분은 0으로 마스킹한다.\n",
    "    segment = [0]*SEQ_LEN  # 문장의 길이만큼 segment를 0으로 채운다.\n",
    "\n",
    "    tokens.append(token)\n",
    "    segments.append(segment)\n",
    "    masks.append(mask)\n",
    "\n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    return [tokens, masks, segments]\n",
    "\n",
    "\n",
    "def review_evaluation_predict(sentence):\n",
    "    data_x = sentence_convert_data(sentence)\n",
    "    predict = sentiment_model.predict(data_x)\n",
    "    predict_value = np.ravel(predict)\n",
    "    predict_answer = np.round(predict_value, 0).item()\n",
    "\n",
    "    if predict_answer == 0:\n",
    "      # 예측값이 0일 경우, 부정적인 평가로 판단한다.\n",
    "      print(\"(부정 확률 : %.2f) 부정적인 평가입니다.\" % (1-predict_value))\n",
    "    elif predict_answer == 1:\n",
    "      # 예측값이 1일 경우, 긍정적인 평가로 판단한다.\n",
    "      print(\"(긍정 확률 : %.2f) 긍정적인 평가입니다.\" % predict_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.save_weights(\"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'밑반찬도 맛있고 특히 장어가 맛있어요'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_star[df_no_star['review_content'].notnull()].iloc[0,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
